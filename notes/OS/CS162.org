#+TITLE: Operating Systems and System Programming
#+OPTIONS: num:nil toc:nil

* Template

** Header:
   - Date: 

** Key Points:

** Notes:

** Summary:


* What is an Operating System?

** Header:
   - Date: 22 March 2019

** Key Points:
   Three roles of OS:
   - Referee
     
     Manage resources, Procetion, Isolation

   - Illusionist

     Provide abstractions of physical resources, VM

   - Glue
     
     Common services

** Notes:
   - OS Concepts: How to Navigate as a Systems Programmer

     Process, I/O, Networks and VM.

   - Concurrency

     Threads, scheduling, locks, deadlock, etc.

   - Address Space

     Virtual memory, address translation, protection, sharing

   - File Systems

     I/O devices, file objects, storage, naming, caching,
     performance, paging, transactions, databases

   - Distributed Systems

     Protocols, N-Tiers, RPC, NFS, DHTs, Consistency, Scalability,
     multicast

   - Reliability & Security

     Fault tolerance, protection, security

** Summary:
   - OS provide a VM abstraction to handle diverse hw
   - OS coordinate resources and protect users from each other
   - OS simplify apps dev by providing standard services
   - OS can provide an array of fault containment, fault tolerance,
     and fault recovery


* Introduction to the Process

** Header:
   - Date: 23 March 2019

** Key Points:
   - Thread
     + Single unique execution context.
     + Program Counter, Registers, Execution Flags, Stack.
   - Address Space w/ Translation
     + Programs execute in an address space that is distinct from the
       memory space of the physical machine.
   - Process
     + An instance of an executiong program is a process consisting of
       an address space and one or more threads of control.
   - Dual mode operation/Protection
     + Only the "system" has the ability to access certain resources
     + The OS and the hardware are protected from user programs and
       user programs are isolated from each other by controlling the
       translation from program virtual addresses to machine physical
       addresses.
       
** Notes:
*** Thread
    - Single unique execution context.
      + Program Counter(PC), Registers, Execution Flags, Stack.
    - PC register holds the address of executing instruction in the
      thread.
    - Certain registers hold the _context_ of thread.
      + Stack pointer holds the address of the top of stack.

*** Address Space
    - Programs execute in an address space that is distinct from the
      memory space of the physical machine.
    - Each virtual "CPU" needs a structure to hold:
      + Program Counter (PC), Stack Poiter(SP).
      + Registers (Integer, Floating point, others...).
    - To switch from one vCPU to the next:
      + Save PC, SP, and registers in current state block.
      + Load PC, SP, and registers from new state block.
    - Switch triggers by:
      + Timer, voluntary yield, I/O, ...

*** Process
    - Execution environment with Restricted Rights:
      + Address Space with One or More Threads
      + Owns memory (address space)
      + Encapsulate one or more threads sharing process resources
    - Why processes?
      + Protected from each other!
      + OS Protected from them
      + Navigate fundamental tradeoff between protection and
        efficiency
      + Processes provides memory protection
      + Threads more efficient than processes
    - Application instance consists of one or more processes

*** Protection
    - OS must protect itself from user programs:
      + *Reliability*: compromising the operating system generally
        causes it to crash.
      + *Security*: limit the scope of what processes can do.
      + *Privacy*: limit each process to the data it is permitted to access
      + *Fairness*: each should be limited to its appropriate share.

      + Only the "system" has the ability to access certain resources.
      + The OS and the hardware are protected from user programs and
	user programs are isolated from each other by controlling the
	translation from program virtual addresses to machine physical
	addresses.
    - OS must protect User programs from each other.
    - Primary Mechanism: limit the translation from program address
      space to physical memory space.
      + Can only touch what is mapped in.
    - Additional Mechanisms:
      + Privileged instructions, in/out instructions, special
        registers.
      + syscal processing, subsystem implementation.

*** Dual Mode operation
    - *Hardware* provides at least two modes:
      + "Kernel" mode ("supervisor", "protected")
      + "User" mode: Normal programs executed
    - Hardware to support "dual mode" operation
      + a bit of state (user/system mode bit)
      + Certain operations/ actions only permitted in kernel mode
    - User -> kernel transition sets system mode AND saves the user
      state
      + OS code carefully puts aside user state then performs the
        necessary operation
    - Kernel -> User transition clears system mode AND restore state
      + return-from-interrupt

*** Mode Transfer
    - Syscall
    - Interrupt
    - Trap or Exception

** Summary:


* Processes, Fork, Introduction to I/O

** Header:
   - Date: 23 March 2019

** Key Points:
   1. Process Contorl Block(PCB) stores all proces info.
   2. Scheduler decide which process/thread execute.
   3. Every process has it's own kernel stack.
   4. Unix has uniform syscall 'API'.

** Notes:
*** Process Contorl Block(PCB)
    - Kernel represents each process as a PCB
      + Status (running, ready, blocked, ...)
      + Register state
      + Process ID(pid), User, Executable, Priority, ...
      + Execution time, ...
      + Memory space, translation, ...
    - Kernel *Scheduler* maintains a data structure containg the PCBs.
    - Scheduling algorithm selects the next one to run.
*** Scheduler
    - Scheduling: Mechanism for deciding which processes/threads
      receive the CPU.
*** Kernel stack
    - Kernel needs space to work.
    - Cannot put anything on the user stack.
    - Two-stack model
      + OS thread has interrupt stack(located in kernel memory) plus
        User stack(located in user memory).
      + Syscall handler copies user args to kernel space before
        invoking specific function.
*** Safely Interrupts
    - *Interrupt vector*
      + Limited number of entry points into kernel.
    - Kernel interrupt stack
      + Handler works regardless of state of user code.
    - Interrupt masking
      + Handler is non-blocking.
    - Atomic transfer of control
      + "Single instruction" - like to change:
	+ Program counter.
	+ Stack pointer.
	+ Memory protection.
	+ Kernel/user mode.
    - Transparent restartable execution
      + User program does not know interrupt occurred.
*** Key Unix I/O Design Concepts
    - Uniformity
      + file operations, device I/O, and ipc through open, read/write,
        close.
      + Allows simple composition of programs.
    - Open before use
      + Provides opportunity for access control and arbitration.
      + Sets up the underlying machinery, i.e., data strcutures.
    - Byte-oriented
      + Even if blocks are transferred, addressing is in bytes.
    - Kernel buffered reads
      + Streaming and block devices looks the same.
      + Read blocks process, yielding processor to other task.
    - Kernel buffered writes
      + Completion of out-going transfer decoupled from the
        application, allowing it to continue.
    - Explicit close
*** The file system abstraction
    - File
      + Named collection of data in a file system
      + File data
      + File Metadata: information about the file
    - Directory
      + "Folder" containing files & Directories
      + Hierachical nameing
      + Links and Volumes

** Summary:
*** Interrupts
    - Hardware mechanism for regaining control from user.
    - Notification that events have occurred.
    - User-level equivalent: Signals.

*** Native control of Process
    - Fork, Exec, Wait, Signal.

*** Basic Support for I/O
    - Standard interface: open, read, write, seek.
    - Device drivers: customized interface to hardware.


* I/O, Sockets, Networking

** Header:
   - Date: 25 March 2019

** Key Points:
   - STDIN/STDOUT enable composition in Unix
     + Use of pipe symbols connects STDOUT and STDION
   - Device Driver: Device-specific code in the kernel that interacts
     directly with the device hardware
     + Supports a standard, internal interface.
     + Same kernel I/O system can interact easily with different
       device drivers.
   - File abstraction works for inter-processes communication
     + Can work across the Internet
   - Socket: an abstraction of a network I/O queue
     + Mechanism for inter-process communication.

** Notes:
*** Socket
    - *Socket*: an abstraction of a network I/O queue
      + Mechanism for inter-process communication.
      + Embodies one side of a communication channel
	* Same interface regardless of location of other end
	* Could be local machine("UNIX socket") or remote
          ("network socket").
    - Data transfer like files
      + Read/Write against a descriptor
    - Over ANY kind of network
      + Local to a machine
      + Over the internet (TCP/IP, UDP/IP)
      + OSI, Appletalk, SNA, IPX, SIP, NS, ...

** Summary:
*** BIG OS Concepts
    - Processes
    - Address Space
    - Protection
    - Dual Mode
    - Interrupt handlers (including syscall and trap)
    - File System
      + Integrates processes, users, cwd, protection.
    - Key Layers: OS Lib, Syscall API, Subsystem, Driver
      + User handler on OS descriptors.
    - Process control
      + fork, wait, signal, exec.
    - Communication through sockets


* Networking, Concurrency(Processes and Threads)

** Header:
   - Date: 26 March 2019

** Key Points:

** Notes:
*** Thread State
    - State shared by all threads in process/addr space
      + Content of memory (global variables, heap)
      + I/O state (file descriptors, network connections, etc)
	
    - State "private" to each thread
      + Kept in TCB = Thread Control Block
      + CPU registers (including PC)
      + Execution stack
    
    - Execution stack
      + Parameters, temp variables
      + Return PCs are kept while called procedure are executing

** Summary:
   - Processes have two parts
     + Threads (Concurrency)
     + Address Space (Protection)
   - Concurrency accomplished by multiplexing CPU Time:
     + Unloading current thread (PC, registers).
     + Loading new thread (PC, registers).
     + Such context switching may be voluntary (yield(), I/O
       operations) or involuntary (timer, other interrupts).
   - Protection accomplished restricting access:
     + Memory mapping isolates processes from each other.
     + Dual-mode for isolating I/O, other resources.
   - Various Textbooks talk about *processes*
     + When this concerns concurrency, really talking about thread
       portion of a process.
     + When this concerns protection, talking about address space
       portion of a process.


* Concurrency, Synchronization

** Header:
   - Date: 27 March 2019

** Key Points:

** Notes:
   
** Summary:


* Synchronization

** Header:
   - Date: 30 March 2019

** Key Points:
   
** Notes:
   
** Summary:
*** Important concept: Atomic Operations
    - An operation that runs to completion or not at all.
    - Thes are the primitives on which to construct various
      synchronization primitives.
*** Hardware atomicity primitives:
    - Disabling of Interrupts, test&set, swap, comp&swap,
      load-linked/store conditional.


* Semaphores, Monitors, and Readers/Writers

** Header:
   - Date: 31 March 2019
     
** Key Points:

** Notes:
*** Higher-level Primitives than Locks
**** Semaphores
     A *Semaphore* has a non-negative integer value and supports the
     follwing two operations:
     - P(): an atomic operation that waits for semaphore to become
       posivie, then decrements it by 1.
     - V(): an atomic operation that increments the semaphore by 1,
       waking up a waiting P, if any.
**** Monior
     A *Monitor* is a lock and zero or more condition variables for
     managing concurrent access to shared data.
**** Condition Variable
     A *Condition Variable* is a queue of threads waiting for
     something inside a critical section.
     - Key idea: allow sleeping inside critical section by atomically
       releasing lock at time we go to sleep.
       
       
     Operations:
     - Wait(&lock): Atomically release lock and go to sleep.
       Re-acquire lock later, before returning.
     - Signal(): Wake up one waiter, if any.
     - Broadcast(): Wake up all waiters.

** Summary:
   

* Readers/Writers

** Header:
   - Date: 31 March 2019

** Key Points:

** Notes:
   State variables:
   - int AR: Number of active readers; initially = 0.
   - int WR: Number of waiting readers; initially = 0.
   - int AW: Number of active writers; initially = 0.
   - int WW: Number of waiting writers; initially = 0.
   - Condition okToRead = NIL.
   - Condition okToWrite = NIL.

*** Reader
    #+BEGIN_SRC c
      Reader()
      {
	lock.Acquire();

	while((AW + WW) > 0)
	{
	  WR++;
	  okToRead.wait(&lock);
	  WR--;
	}
	AR++;
	lock.Release();
  
	// Perform actual read-only access
	AccessDatabase(ReadOnly);

	lock.Acquire();
	AR--;
	if(AR == 0 && WW > 0)
	  okToWrite.signal();
	lock.Release();
      }
    #+END_SRC
*** Writer
    #+BEGIN_SRC c
      Writer()
      {
	lock.Acquire();
	while((AW + AR) > 0)
	{
	  WW++;
	  okToWrite.wait(&lock);
	  WW--;
	}
	AW++;
	lock.Release();
  
	// Perform actual read/write access
	AccessDatabase(ReadWrite);

	lock.Acquire();
	AW--;
	if(WW > 0)
	{
	  okToWrite.signal();
	}
	else if(WR > 0)
	{
	  okToRead.broadcast();
	}
	lock.Release();
      }
    #+END_SRC
    
** Summary:
*** Semaphores: Like integers with restricted interface
     - Two ops:
       + P(): Wait if zero; dec when becomes non-zero
       + V(): Inc and wake a sleeping task
       + Can initialize value to any non-negative value
     - Use separate semaphore for each constraint
*** Monitors: A lock plus one or more condition variables
    - Always acquire lock before accessing shared data
    - Use condition variables to wait inside critical section
      + Ops: Wait(), Signal(), and Broadcast(). 


* Scheduling

** Header:
   - Date: 1 April 2019

** Key Points:

** Notes:
*** Round Robin (RR)
**** FCFS(FIFO): Potentially bad for shord jobs!
     - Depends on submit order

**** Round Robin Sceme
     - Each process gets a small unit of CPU time (time quantum),
       usually 10-100 miliseconds
     - After quantum expires, the process is preempted and added to
       the end of thi ready queue.
     - /n/ processes in ready queue and time quantum is q->
       + Each process gets 1/n of the CPU time
       + In chunks of at most /q/ time units
       + No process waits more than (n-1)q time units
**** Performance
     - if q large -> FCFS
     - if q small -> Interleaved
     - q must be large with respect to context switch, otherwise
       overhead is too high (all voerhead)

** Summary:
   - Round-Robin (RR) Scheduling:
     + Give each thread a small amount of CPU time when it executes;
       cycle between all ready threads.
     + Pros: Better for short jobs.
   - Shortest Job First (SJF)/Shortest Remaining Time First(SRTF):
     + Run whatever job has the least amount of computations to
       do/least remaining amount of computation to do.
     + Pros: Optimal.
     + Cons: Hard t opredict future, Unfair.
   - Multi-Level Feedback Scheduling:
     + Multiple queues of different priorities and scheduling
       algorithms.
     + Automatic promotion/demotion of process priority in order to
       approximate SJF/SRTF.
   - Loterry Scheduling:
     + Give each thread a priority-dependent number of tokens(short
       task -> more tokens).
   - Linux CFS Scheduler: Fair fraction of CPU
     + Approximates a "ideal" multitasking processor.
   - Realtime Schedulers such as EDF
     + Guaranteed behavior by meeting deadlines.
     + Realtime tasks defined by tuple of compute time and period.
     + Schedulability test: is it possible to meet deadlines with
       proposed set of processes?


* Deadlock, Address Translation

** Header:
   - Date: 2 April 2019

** Key Points:

** Notes:
*** Foru requirements for Deadlock
    - Mutual exclusion
      + Only one thread at a time can use a resource.
    - Hold and wait
      + Thread holding at least one resource is waiting to acquire
        additional resources held by other threads.
    - No preemption
      + Resources are released only voluntarily by the thread holding
        the resource, after thread is finished with it.
    - Circular wait

** Summary:
   - Starvation vs. Deadlock
     + Starvation: thread waits indefinitely
     + Deadlock: circular waiting for resources
   - Techniques for addressing Deadlock
     + Allow system to enter deadlock and then recover
     + Ensure that system will never ender a deadlock
     + Ignore the problem and pretend that deadlocks never occur in
       the system
       
       
   - Memory is a resource that must be multiplexed
     + Controlled Overlap: only shared when appropriate
     + Translation: Change virtual addresses into physical addresses
     + Protection: Prevent unauthorized sharing of resources
   - Simple Protection through segmentation
     + Base + Limit registers restrict memory accessible to user
     + Can be used to translate as well
   - Page Tables
     + Memory divided into fixed-size chunks of memory
     + Offest of virtual address same as physical address


* Address Translation

** Header:
   - Date: 5 April 2019

** Key Points:

** Notes:
*** Paging
    - Page Table (One per process)
      + Resides in physical memory.
      + Contains physical page and permission for each virtual page.
    - Virtual address mapping
      + Offset from Virtual address copied to Physical Address.
      + Virtual page # is all remaining bits.
      + Check Page Table bounds and permissions.
*** Two-Level Page Table
    - Tree of Page Tables
    - Tables fixed size
      + On context-switch save single PageTablePtr register.
    - Valid bits on Page Table Entries
      + Don't need every 2nd-level table
      + Even when exist, 2nd-level tables can reside on disk if ton in
        use
*** Page Table Entry(PTE)
    - Pointer to next-level page table or to actual page
    - Permission bits: valid, read-only, read-write, write-only
*** Intel x86 arch PTE:
    - 10, 10, 12-bit offset
    - Intermediate page tables called "Directiries"


    | Page Frame Number | Free(OS) | O | L | D | A | PCD | PWT | U | W | P |
    |-------------------+----------+---+---+---+---+-----+-----+---+---+---|
    |             31-12 |     11-9 | 8 | 7 | 6 | 5 |   4 |   3 | 2 | 1 | 0 |
    - P: Present (same as "valid")
    - W: Writeable
    - U: User accessible
    - PWT: Page Write Transparent: external cache write-through
    - PCD: Page cache disabled (page cannot be cached)
    - A: Accessed: page has been accessed recently
    - D: Dirty (PTE only): page has been modified recently
    - L: L=1 -> 4MB page (directory only)

*** UNIX fork and COPY on Write

*** Segments + Pages
    
*** Address Translation(Virtual Memory) Comparison
    |                     | Advantages                   | Disadvantages          |
    |---------------------+------------------------------+------------------------|
    | Simple segmentation | Fast context switching       | External fragmentation |
    | Single-level Paging | No fragm. fast               | Large table size       |
    | Paged segmentation  | Table size ~# of pages in vm | Multiple memeory       |
    | Two-level Paging    | same as prev                 | same as prev           |
    | Inverted Table      | Table size ~# of pages in pm | Hash func more complex |

*** How to get from Kernel->User
    - What does the kernel do to create a new user porcess?
      + Allocate and init address space control block
      + Read program off disk and store in memory
      + Allocate and init translation table
      + Run Program:
	* Set machine registers
	* Set hardware pointer to translation table
	* Set processor status word for user mode
	* Jump to start of program
    - How does kernel switch between processes?
      + Same saving/restoring of registers as before
      + Save/restore PSL(Hardware pointer to translation table)

** Summary:
   - Page Tables
     + Memory divided into fixed-sized chunks of memory
     + Virtual page number from virtual address mapped through page
       table to physical page number
     + Offset of the virtual address same as physical address
     + Large page table can be plased into virtual memory
   - Multi-Level Tables
     + Virtual address mapped to series of tables
     + Permit sparse population of address space
   - Inverted page table
     + Size of page table related to physical memory size
   - PTE: Page Table Entries
     + Includes physical page number
     + Control info (valid bit, writeable, dirty, user, etc)


* Cache

** Header:
   - Date: 5 April 2019 

** Key Points:

** Notes:
*** Cache
    - *Cache*: a repository for copies that can be accessed more
      quickly than the original
      + Make frequent case fast and infrequent case less dominant
    - Caching underlies many of the techniques that are used today to
      make computers fast
    - Important measure: Average Access time =
      (Hit rate * Hit Time) + (Miss Rate * Miss time)

*** Translation Cache: TLB(Translation Lookaside Buffer)

*** Locality
    - Temporal Tocality (Locality in Time):
      + Keep recently accessed data items closer to processor.
    - Spatial Locality (Locality in Space):
      + Move contiguous blocks to the upper levels

*** Sources of Cache Misses
    - Compulsory (cold start or process migration, first reference):
      first access to a block
      + "Cold" fact of life: not a whole lot you can do about it.
      + Note: If you are going to run "billions" of instruction,
        Compulsory Misses are insignificant
    - Capacity:
      + Cache cannot contain all blocks access by the program
      + Solution: increase cache size
    - Conflict (collision):
      + Multiple memory locations mapped to the same cache location
      + Solution 1: increase cache size
      + Solution 2: increase associativity
    - Coherence (invalidation): other process updates memory

** Summary:
   - The Principle of Locality:
     + Program likely to access a relatively small portion of the
       address space at any instant of time
       * Temporal Locality: Locality in Time
       * Spatial Locality: Locality in Space
   - Three (+1) Major Categories of Cache Misses:
     + Compulsory Misses: sad facts of life.
     + Conflict Misses: increase cache size and/or associativity
     + Capacity Misses: increase cache size
     + Coherence Misses: Caused by external processors or I/O devices
   - Cache Organizations:
     + Direct Mapped: single block per set
     + Set associative: more than one block per set
     + Fully associative: all entries equivalent
